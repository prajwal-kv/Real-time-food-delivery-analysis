Real-Time Food Data Analysis Pipeline
    
A production-ready, real-time data analytics pipeline for processing and analyzing food industry data using AWS services, Apache Airflow, and PySpark.

ğŸ“‹ Table of Contents
â€¢	Overview
â€¢	Architecture
â€¢	Features
â€¢	Tech Stack
â€¢	Prerequisites
â€¢	Project Structure
â€¢	Setup Instructions
â€¢	Pipeline Flow
â€¢	Configuration
â€¢	Usage
â€¢	Monitoring
â€¢	Contributing
â€¢	License

ğŸ¯ Overview

This project implements an end-to-end real-time data pipeline for food industry analytics. It ingests streaming data, processes it using distributed computing, stores it in a scalable data warehouse, and provides interactive dashboards for business intelligence.

Key Capabilities

â€¢	Real-time data ingestion from multiple sources
â€¢	Automated data deduplication and transformation
â€¢	Scalable processing using Apache Spark on EMR
â€¢	Centralized data warehouse with dimensional modeling
â€¢	Interactive dashboards for data visualization
â€¢	CI/CD integration for automated deployments

ğŸ—ï¸ Architecture

Local Development â†’ CodeBuild â†’ S3
                                 â†“
Python Mock Generator â†’ Kinesis Data Stream
                                 â†“
                         Apache Airflow (Orchestration)
                                 â†“
                         Amazon EMR (PySpark)
                          (Deduplication)
                                 â†“
                         Amazon Redshift
                      (Dim Tables + Fact Tables)
                                 â†“
                         Amazon QuickSight
                           (Dashboards)
                           
Architecture Components

1.	CI/CD Layer: Automated deployment of DAGs, PySpark scripts, and dimension tables
2.	Ingestion Layer: Real-time data streaming via Kinesis
3.	Orchestration Layer: Workflow management using Apache Airflow
4.	Processing Layer: Distributed data processing with PySpark on EMR
5.	Storage Layer: Data warehouse built on Amazon Redshift
6.	Visualization Layer: Business intelligence dashboards with QuickSight
7.	Security Layer: IAM roles and policies across all services

âœ¨ Features

â€¢	âš¡ Real-time Processing: Stream processing with sub-second latency
â€¢	ğŸ”„ Automated Workflows: Self-managing pipelines with Airflow
â€¢	ğŸ§¹ Data Quality: Built-in deduplication and validation
â€¢	ğŸ“Š Dimensional Modeling: Star schema for optimized analytics
â€¢	ğŸ¨ Interactive Dashboards: Real-time visualization with QuickSight
â€¢	ğŸš€ CI/CD Integration: Automated deployments with AWS CodeBuild
â€¢	ğŸ”’ Secure by Design: IAM-based security across all components
â€¢	ğŸ“ˆ Scalable: Auto-scaling EMR clusters for varying workloads

ğŸ› ï¸ Tech Stack

AWS Services

â€¢	Amazon Kinesis Data Streams: Real-time data ingestion
â€¢	Amazon EMR: Managed Hadoop/Spark cluster
â€¢	Amazon Redshift: Cloud data warehouse
â€¢	Amazon S3: Object storage for scripts and data
â€¢	AWS CodeBuild: CI/CD automation
â€¢	Amazon QuickSight: Business intelligence and visualization
â€¢	AWS IAM: Security and access management

Frameworks & Tools

â€¢	Apache Airflow: Workflow orchestration
â€¢	Apache Spark (PySpark): Distributed data processing
â€¢	Python: Data generation and scripting
â€¢	SQL: Data querying and transformation

ğŸ“¦ Prerequisites

â€¢	AWS Account with appropriate permissions
â€¢	Python 3.8+
â€¢	Apache Airflow 2.x
â€¢	Basic knowledge of: 
o	AWS services
o	Apache Spark
o	SQL
o	Python

Required AWS Permissions

â€¢	EMR: Full access for cluster management
â€¢	Kinesis: Read/Write access to data streams
â€¢	Redshift: Admin access for database operations
â€¢	S3: Read/Write access to buckets
â€¢	IAM: Role creation and policy attachment
â€¢	CodeBuild: Project management
â€¢	QuickSight: Dashboard creation and management

ğŸ“ Project Structure

food-data-pipeline/
â”œâ”€â”€ dags/

â”‚   â”œâ”€â”€ food_data_pipeline_dag.py

â”‚   â””â”€â”€ config/

â”‚       â””â”€â”€ redshift_config.py

â”œâ”€â”€ scripts/

â”‚   â”œâ”€â”€ data_generator/

â”‚   â”‚   â””â”€â”€ mock_food_data_generator.py

â”‚   â”œâ”€â”€ spark_jobs/

â”‚   â”‚   â””â”€â”€ food_data_processor.py

â”‚   â””â”€â”€ redshift/

â”‚       â”œâ”€â”€ create_dim_tables.sql
â”‚       â””â”€â”€ create_fact_tables.sql
â”œâ”€â”€ config/

â”‚   â”œâ”€â”€ emr_config.json
â”‚   â””â”€â”€ kinesis_config.json

â”œâ”€â”€ buildspec.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ Setup Instructions

1. Clone the Repository

git clone https://github.com/yourusername/food-data-pipeline.git
cd food-data-pipeline

2. Set Up AWS Resources

Create S3 Buckets
aws s3 mb s3://food-pipeline-scripts
aws s3 mb s3://food-pipeline-data
Create Kinesis Data Stream
aws kinesis create-stream \
    --stream-name food-data-stream \
    --shard-count 2
Create Redshift Cluster
aws redshift create-cluster \
    --cluster-identifier food-data-warehouse \
    --node-type dc2.large \
    --master-username admin \
    --master-user-password YourPassword123 \
    --number-of-nodes 2
Create EMR Cluster
aws emr create-cluster \
    --name "Food Data Processing Cluster" \
    --release-label emr-6.10.0 \
    --applications Name=Spark Name=Hadoop \
    --ec2-attributes KeyName=your-key-pair \
    --instance-type m5.xlarge \
    --instance-count 3 \
    --use-default-roles
    
3. Configure IAM Roles

Create IAM roles for:

â€¢	EMR service role
â€¢	EMR EC2 instance profile
â€¢	Airflow execution role
â€¢	Redshift access role
4. Set Up Apache Airflow
# Install Airflow
pip install apache-airflow[amazon,postgres]

# Initialize Airflow database

airflow db init

# Create admin user

airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
    
5. Configure Environment Variables

Create a .env file:
AWS_REGION=us-east-1
KINESIS_STREAM_NAME=food-data-stream
EMR_CLUSTER_ID=j-XXXXXXXXXXXXX
REDSHIFT_HOST=your-cluster.region.redshift.amazonaws.com
REDSHIFT_DATABASE=fooddata
REDSHIFT_USER=admin
REDSHIFT_PASSWORD=YourPassword123
S3_SCRIPTS_BUCKET=food-pipeline-scripts
S3_DATA_BUCKET=food-pipeline-data

6. Deploy with CodeBuild

# Create CodeBuild project
aws codebuild create-project \
    --name food-pipeline-build \
    --source type=GITHUB,location=https://github.com/yourusername/food-data-pipeline.git \
    --artifacts type=S3,location=food-pipeline-scripts \
    --environment type=LINUX_CONTAINER,image=aws/codebuild/standard:5.0,computeType=BUILD_GENERAL1_SMALL \
    --service-role your-codebuild-role-arn
    
ğŸ”„ Pipeline Flow

End-to-End Data Flow

1.	Data Generation
o	Python mock generator creates realistic food industry data
o	Data includes orders, inventory, sales, and customer information
2.	Data Ingestion
o	Generator pushes data to Kinesis Data Stream
o	Stream buffers data for real-time processing
3.	Orchestration
o	Airflow DAG triggers based on schedule or event
o	Airflow submits EMR step for Spark job execution
4.	Data Processing
o	EMR reads data from Kinesis stream
o	PySpark performs: 
ï‚§	Data deduplication
ï‚§	Schema validation
ï‚§	Data transformation
ï‚§	Aggregations
5.	Data Loading
o	Dimension tables loaded from S3 (via CI/CD)
o	Fact tables loaded from EMR processing results
o	Data inserted into Redshift tables
6.	Visualization
o	QuickSight connects to Redshift
o	Dashboards display real-time analytics
o	Automated report generation

âš™ï¸ Configuration

Airflow DAG Configuration
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'food_data_pipeline',
    default_args=default_args,
    description='Real-time food data processing pipeline',
    schedule_interval='@hourly',
    catchup=False
)
EMR Spark Job Configuration
spark_config = {
    'spark.streaming.stopGracefullyOnShutdown': 'true',
    'spark.streaming.backpressure.enabled': 'true',
    'spark.sql.adaptive.enabled': 'true',
    'spark.dynamicAllocation.enabled': 'true'
}

ğŸ“Š Usage
Start Data Generation
python scripts/data_generator/mock_food_data_generator.py \
    --stream-name food-data-stream \
    --records-per-batch 100 \
    --interval 5
    
Trigger Airflow DAG

# Via CLI
airflow dags trigger food_data_pipeline

# Via UI
# Navigate to http://localhost:8080 and enable the DAG

Query Redshift

-- Example: Get daily sales summary
SELECT 
    date_key,
    product_name,
    SUM(quantity) as total_quantity,
    SUM(revenue) as total_revenue
FROM fact_sales fs
JOIN dim_product dp ON fs.product_key = dp.product_key
JOIN dim_date dd ON fs.date_key = dd.date_key
GROUP BY date_key, product_name
ORDER BY date_key DESC;
View QuickSight Dashboard
1.	Log into AWS QuickSight console
2.	Navigate to Dashboards
3.	Select "Food Data Analytics Dashboard"
4.	View real-time metrics and trends

ğŸ“ˆ Monitoring

CloudWatch Metrics
â€¢	Kinesis stream metrics (incoming records, iterator age)
â€¢	EMR cluster metrics (CPU, memory, tasks)
â€¢	Redshift query performance
â€¢	Airflow task success/failure rates

Airflow Monitoring
# Check DAG status
airflow dags list

# View task logs

airflow tasks logs food_data_pipeline task_name 2024-01-01
EMR Monitoring

â€¢	Access EMR cluster UI at port 8088
â€¢	Monitor Spark jobs and stages
â€¢	Check YARN resource manager

ğŸ¤ Contributing
Contributions are welcome! Please follow these steps:
1.	Fork the repository
2.	Create a feature branch (git checkout -b feature/AmazingFeature)
3.	Commit your changes (git commit -m 'Add some AmazingFeature')
4.	Push to the branch (git push origin feature/AmazingFeature)
5.	Open a Pull Request

Coding Standards

â€¢	Follow PEP 8 for Python code
â€¢	Add docstrings to functions and classes
â€¢	Write unit tests for new features
â€¢	Update documentation as needed

ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ‘¥ Authors
â€¢	Your Name - Prawjal KV

ğŸ™ Acknowledgments
â€¢	AWS Documentation
â€¢	Apache Airflow Community
â€¢	Apache Spark Community

ğŸ“ Support
For questions or issues:
â€¢	Open an issue in GitHub
â€¢	Email: prajwalkv620@gmail.com

